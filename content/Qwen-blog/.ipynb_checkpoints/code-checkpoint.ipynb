{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2学习笔记\n",
    "\n",
    "跟着tiny-universe这个项目打算把一些模型都写一遍。首先写的就是Qwen2，这是记录Qwen2的学习内容。\n",
    "\n",
    "## 主模型类：Qwen2Model\n",
    "Qwen的主题架构如下图所示：\n",
    "![框架图](./img/framework.JPEG)  \n",
    "\n",
    "其中的主架构就分为三部分：\n",
    "- tokenizer:将文本转为词表索引的类\n",
    "- Embedding：将索引转为对应向量的类\n",
    "- Layers:最主要的部分，相当于Transformer的解码器，有很多个Qwen2Decoder类\n",
    "- RMSNorm：一个规范化方法\n",
    "\n",
    "剩下的Linear，Loss，Output其实就不算模型内的了，它会根据训练和推理的不同而选择不同的处理办法。\n",
    "\n",
    "先看主模型的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# Qwen2的主模型\n",
    "class Qwen2Model(nn.Model):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(self.config.vocab_size,self.config.hidden_size,self.config.padding_idx)\n",
    "        self.layers = nn.ModuleList([Qwen2DecoderLayer(config,layer_idx)\n",
    "                                     for layer_idx in range(self.config.num_hidden_layers)])\n",
    "        self.norm = Qwen2RMSNorm(self.config.hidden_size,eps=self.config.rms_norm_eps)\n",
    "        self.gradient_checkpointing = False\n",
    "        self.post_init()\n",
    "    def post_init(self):\n",
    "        self.init_weights()\n",
    "        self._backward__compatibility_gradient_checkpointing()\n",
    "        \n",
    "    \n",
    "    def forward(self,input_ids,position_ids):\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        hidden_states = inputs_embeds\n",
    "        if self.config.output_hidden_states:\n",
    "            all_hidden_states = ()\n",
    "        for idx,decoder_layer in enumerate(self.layers):\n",
    "            if self.config.output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask = self.config.attention_mask,\n",
    "                position_ids = position_ids,\n",
    "                past_key_value = self.config.past_key_value,\n",
    "                output_attentions = self.config.output_attentions,\n",
    "                use_cache = self.config.use_cache,\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "        hidden_states = self.norm(hidden_states)    \n",
    "        if self.config.output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "            return all_hidden_states\n",
    "        else:\n",
    "            return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主模型中的的初始化参数就一个config，其实这也是一个类，是Qwen2Config类。基本上用的到的参数都写在了这个类中。所以想要了解Qwen2需要的参数，最好去看这个类。当然了，是看完其他知识点后再去看，不然看到参数什么也不懂。\n",
    "\n",
    "主模型初始化中只创建了三个类：Embedding，layers和norm,分别对应Embedding，layers和RMSNorm。\n",
    "\n",
    "Embedding需要一个词表大小vocab_size和词元维度dim，还有一个可选的填充字符在词表中的索引padding_idx。\n",
    "layers本身其实就是一个数组，里面存放了很多个Qwen2Decoder类\n",
    "RMSNorm就是一个简单的规范化类\n",
    "\n",
    "整个的执行逻辑就是一个简单的线性执行。输入input_ids，即文本词元在词表对应的索引。然后执行Embedding，接着就是Qwen2Decoders，最后一个规范化RMSNorm就行了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码器类：Qwen2Decoder\n",
    "\n",
    "Qwen2Decoder类是Qwen2的核心类。它主要包括四个类：\n",
    "\n",
    "- self_attn:Attention类，用来执行Attention，默认选的是Qwen2Attention\n",
    "- mlp:一个MLP，主要就是全连接层\n",
    "- input_layernorm:规范化类\n",
    "- post_attention_layer:规范化类\n",
    "\n",
    "整个模型的结构图如下：\n",
    "![Qwen2Decoder](./img/decoderlayer.png)\n",
    "\n",
    "整个结构可以说分为两个部分：第一部分就是RMSNorm+Attention+residual；第二部分就是RMSNorm+mlp+residual。也就是说这两部分也就attention和mlp不同。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN2_ATTENTION_CLASSES = {\n",
    "    \"eager\":Qwen2Attention, \n",
    "    \"flah_attention_2\":Qwen2FlashAttention2,\n",
    "    \"sdpa\":Qwen2SdpaAttention,\n",
    "}        \n",
    "class Qwen2DecoderLayer(nn.Module):\n",
    "    def __init__(self,config,layer_idx):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.self_attn =  QWEN2_ATTENTION_CLASSES[config._attn_implementation](config,layer_idx)\n",
    "        self.mlp = Qwen2MLP(config)\n",
    "        self.input_layernorm = Qwen2RMSNorm(config.hidden_size,eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size,eps=config.rms_norm_eps)\n",
    "        \n",
    "    def forward(self,hidden_states,position_ids,attention_mask,past_key_value,output_attentions):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states,self_attn_weights,\\\n",
    "        present_key_value= self.self_attn(\n",
    "            hidden_states = hidden_states,\n",
    "            attention_mask = attention_mask,\n",
    "            position_ids = position_ids,\n",
    "            past_key_value = past_key_value,\n",
    "            output_attentions = output_attentions,\n",
    "            use_cache = self.config.use_cache,\n",
    "        )\n",
    "        hidden_states = hidden_states+residual\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = hidden_states+residual\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力类：Qwen2Attention\n",
    "\n",
    "结构图如下：\n",
    "![Qwen2Attention](./img/Qwen2Attention.png)\n",
    "\n",
    "这是整个Qwen2的重点。\n",
    "\n",
    "首先，它使用的是旋转位置编码，这是一种可以用绝对位置编码方式来实现相对位置编码方式的方法，最大的优点就是不影响attention的优化。因为它是对q，k分别执行位置编码，而不是对$\\frac{q*k^T}{\\sqrt{d}}$执行编码。\n",
    "\n",
    "其次，它使用的不是传统的多头注意力，而是GQA，这会大大降低训练的运算量和内存压力，特别是KV缓存的压力。\n",
    "\n",
    "所以整个程序的重点就两个，一个就是实现GQA，另一个就是实现旋转位置编码RoPE。\n",
    "\n",
    "最后吐槽一下这个图，里面有一个Repeat_kv，这个块实际处理的是Key和Value，所以叫做Repeat_kv，但图上画的却是从Query和Key引下来的，给人感觉是对Query和Key执行Repeat_kv。\n",
    "\n",
    "模型的初始化中主要就四个线性类nn.Linear用来得到Query，Key，Value以及将结果投影回来，另外还有一个Qwen2RotaryEmbedding用来得到位置编码矩阵sin和cos。\n",
    "\n",
    "整个模型的执行顺序没什么说的。\n",
    "\n",
    "1. 先执行输入的投影，即计算query，key，value\n",
    "2. 对query，key，value的形状进行改变，即从原本的[bs,q_len,num_heads*head_dim]转为[bs,num_heads,q_len,head_dim]\n",
    "3. 计算旋转位置矩阵sin，cos\n",
    "4. 使用sin，cos对query，key执行位置编码，得到编码后的query和key\n",
    "5. 对key，value执行repeat_kv,使得key，value的num_heads与query相等\n",
    "6. 执行公式$\\frac{q*k^T}{\\sqrt{d}})$，得到attn_weights\n",
    "7. 对attn_weights执行mask操作\n",
    "8. 执行softmax函数，\n",
    "9. 接着执行dropout\n",
    "10. 然后与value相乘，得到attn_output\n",
    "11. 接着修改attn_output的形状\n",
    "12. 最后将attn_output投影回去，得到最终的结果\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2Attention(nn.Module):\n",
    "    def __init__(self,config,layer_idx):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.is_causal = True\n",
    "        if(self.config.head_dim * self.config.num_heads) != self.config.hidden_size:\n",
    "            raise ValueError(f\"hidden_size must be divisible by num_heads!(got 'hidden_size:'{self.config.hidden_size} and 'num_heads:'{self.config.num_heads})\")\n",
    "        self.q_proj = nn.Linear(self.config.hidden_size,self.config.num_heads*self.config.head_dim,bias=self.config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.config.hidden_size,self.config.num_key_value_heads*self.config.head_dim,bias=self.config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.config.hidden_size,self.config.num_key_value_heads*self.config.head_dim,bias=self.config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.config.num_heads*self.config.head_dim,self.config.hidden_size,bias=self.config.attention_bias)\n",
    "        self.rotary_emb = Qwen2RotaryEmbedding(self.config.head_dim,\n",
    "                    max_position_embeddings=self.config.max_position_embeddings,base=self.config.rope_theta)\n",
    "    def forward(self,hidden_states,attention_mask):\n",
    "        bsz,q_len,_=hidden_states.size()\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        query_states = query_states.view(bsz,q_len,self.config.num_heads,self.config.head_dim).transpose(1,2)\n",
    "        key_states = key_states.view(bsz,q_len,self.config.num_key_value_heads,self.config.head_dim).transpose(1,2)\n",
    "        value_states = value_states.view(bsz,q_len,self.config.key_value_heads,self.config.head_dim).transpose(1,2)\n",
    "        cos,sin = self.rotary_emb(value_states,seq_len=self.config.kv_seq_len)\n",
    "        query_states,key_states = apply_rotary_pos_emb(query_states,key_states,cos,sin,self.config.position_ids)\n",
    "        key_states = repeat_kv(key_states,self.config.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states,self.config.num_key_value_groups)\n",
    "        attn_weights = torch.matmul(query_states,key_states.transpose(2,3))/math.sqrt(self.config.head_dim)\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "        attn_weights = nn.functional.softmax(attn_weights,dim=-1,dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights,p=self.config.attention_dropout,training=self.config.training)\n",
    "        attn_output = torch.matmul(attn_weights,value_states)\n",
    "        attn_output = attn_output.transpose(1,2).contiguous()\n",
    "        attn_output = attn_output.reshape(bsz,q_len,self.config.hidden_size)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 旋转位置编码矩阵：Qwen2RotaryEmbedding\n",
    "\n",
    "旋转位置编码RoPE我这里不作过多的展开讲解，因为确实比较复杂。我建议大家可以看看这篇博客\n",
    "[Transformer升级之路：2、博采众长的旋转式位置编码](https://spaces.ac.cn/archives/8265)\n",
    "\n",
    "创始大佬亲自写的科普文章就问你看不看！！！（苏大佬真的是一个低调的扫地僧，这个科学空间的文章都值得一看）\n",
    "\n",
    "整个论文的思路过程就是求一个编码函数$\\bar{q}_m=f(q,m)$可以对输入的x加上位置m的信息，使其成为$bar{q}_m$。这里的m就是指x在序列中的位置。\n",
    "\n",
    "\n",
    "公式9给出了这个函数的形式，推理的过程就不讲了（我不觉得我会比博客讲的更好）。这里的q就是token本身，是一个向量。公式10则是给出了当q是一个二维向量时的矩阵形式。我这里理解错了，一开始把q0理解成了第一个token，q1理解成了第二个token。q0是q的第一个维度，q1是第二个维度。纠正了这点这个公式就不难理解了。\n",
    "\n",
    "公式11则给出了q是d维时的矩阵形式。公式13则是一个简化计算的形式。\n",
    "\n",
    "总之，按照公式13给token $q$加上位置编码。其中的位置矩阵有两个，sin和cos。这也是Qwen2RotaryEmbedding的主要功能----求取位置编码矩阵。\n",
    "\n",
    "实际的执行其实并不是按照公式13执行的，具体可以看apply_rotary_pos_emb，文字描述好难。具体可以看下图\n",
    "![](./img/ROPE3.png)\n",
    "这个的原理个人感觉应该是q本身是无序的，也就是$q_0,q_1,q_2,...$和$q_0,q_2,q_5...$不应该有区别。所以它可以任意调换位置。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self,dim,max_position_embeddings=2048,base=10000,device=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        # 生成角度theta\n",
    "        inv_freq = 1.0/(self.base)**(torch.arange(0,self.dim,2,dtype=torch.int64).float().to(device)/self.dim)\n",
    "        # 定义一组参数，训练时不会更新，即不会因为optimizer.step()改变（除非动手改变）\n",
    "        # 但在保存模型时会保存这组数据\n",
    "        self.register_buffer(\"inv_freq\",inv_freq,persistent=False)\n",
    "\n",
    "        self._set_cos_sin_cache(seq_len=max_position_embeddings,device=self.inv_freq.device,dtype=torch.get_default_dtype())\n",
    "    \n",
    "    def _set_cos_sin_cache(self,seq_len,device,dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached,device=device,dtype=torch.int64).type_as(self.inv_freq)\n",
    "        # 计算序列序号与角度的内积，即sin和cos内的值\n",
    "        freqs = torch.outer(t,self.inv_freq)\n",
    "        # 因为奇偶是分开的，所以这里重复了一下，这样emb就和x的维度相同了\n",
    "        emb = torch.cat((freqs,freqs),dim=-1)\n",
    "        # 计算cos和sin，并注册数据\n",
    "        self.register_buffer(\"cos_cached\",emb.cos().to(dtype),persistent=False)\n",
    "        self.register_buffer(\"sin_cached\",emb.sin().to(dtype),persistent=False)\n",
    "    \n",
    "    def forward(self,x:torch.Tensor,seq_len=None):\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len,device=x.device,dtype=x.dtype)\n",
    "        return (self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "                self.sin_cached[:seq_len].to(dtype=x.dtype))  \n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[...,:x.shape[-1]//2]\n",
    "    x2 = x[...,x.shape[-1]//2:]\n",
    "    return torch.cat((-x2,x1),dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q,k,cos,sin,position_ids,unsqueeze_dim=1):\n",
    "    \n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q*cos)+(rotate_half(q)*sin)\n",
    "    k_embed = (k*cos)+(rotate_half(k)*sin)\n",
    "    return q_embed,k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接：Qwen2MLP\n",
    "\n",
    "这个全连接类其实没什么讲的，结构图如下：\n",
    "![MLP](./img/MLP1.png)\n",
    "\n",
    "整个MLP的初始化中主要包含了三个线性层和一个激活函数(Act)。这个MLP没有使用线性连接，而是通过一个线性层和激活函数组成了一个门阈结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        \n",
    "        self.gate_proj = nn.Linear(self.hidden_size,self.intermediate_size,bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size,self.intermediate_size,bias=False)\n",
    "        self.down_proj = nn.Linear(self.hidden_size,self.intermediate_size,bias=False)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x))*self.up_proj(x))\n",
    "        return down_proj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 规范化：Qwen2RMSNorm\n",
    "\n",
    "这个类就更没什么说的了，就是执行了一个规范化。和一般的规范化不同的是它只有一个权重weight，没有偏置bias。公式就是\n",
    "$RMSNorm(x)=\\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}w_{i}^{2}+\\epsilon}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2RMSNorm(nn.Module):\n",
    "    def __init__(self,hidden_size,eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "    def forward(self,hidden_states:torch.Tensor):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1,keepdim=True)\n",
    "        hidden_states = hidden_states*torch.rsqrt(variance+self.variance_epsilon)\n",
    "        return self.weight*hidden_states.to(input_dtype)   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是所有的代码了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# Qwen2的模型\n",
    "class Qwen2Model(nn.Model):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(self.config.vocab_size,self.config.hidden_size,self.config.padding_idx)\n",
    "        self.layers = nn.ModuleList([Qwen2DecoderLayer(config,layer_idx)\n",
    "                                     for layer_idx in range(self.config.num_hidden_layers)])\n",
    "        self.norm = Qwen2RMSNorm(self.config.hidden_size,eps=self.config.rms_norm_eps)\n",
    "        self.gradient_checkpointing = False\n",
    "        self.post_init()\n",
    "    def post_init(self):\n",
    "        self.init_weights()\n",
    "        self._backward__compatibility_gradient_checkpointing()\n",
    "        \n",
    "    \n",
    "    def forward(self,input_ids):\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        hidden_states = inputs_embeds\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = ()\n",
    "        for idx,decoder_layer in enumerate(self.layers):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask = self.config.attention_mask,\n",
    "                position_ids = self.config.position_ids,\n",
    "                past_key_value = self.config.past_key_value,\n",
    "                output_attentions = self.config.output_attentions,\n",
    "                use_cache = self.config.use_cache,\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "        hidden_states = self.norm(hidden_states)    \n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "            return all_hidden_states\n",
    "        else:\n",
    "            return hidden_states\n",
    "# 定义了用哪种attention，默认是eager\n",
    "QWEN2_ATTENTION_CLASSES = {\n",
    "    \"eager\":Qwen2Attention, \n",
    "    \"flah_attention_2\":Qwen2FlashAttention2,\n",
    "    \"sdpa\":Qwen2SdpaAttention,\n",
    "}        \n",
    "class Qwen2DecoderLayer(nn.Module):\n",
    "    def __init__(self,config,layer_idx):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.self_attn =  QWEN2_ATTENTION_CLASSES[config._attn_implementation](config,layer_idx)\n",
    "        self.mlp = Qwen2MLP(config)\n",
    "        self.input_layernorm = Qwen2RMSNorm(config.hidden_size,eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size,eps=config.rms_norm_eps)\n",
    "        \n",
    "    def forward(self,hidden_states,position_ids,attention_mask,past_key_value,output_attentions):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states,self_attn_weights,\\\n",
    "        present_key_value= self.self_attn(\n",
    "            hidden_states = hidden_states,\n",
    "            attention_mask = attention_mask,\n",
    "            position_ids = position_ids,\n",
    "            past_key_value = past_key_value,\n",
    "            output_attentions = output_attentions,\n",
    "            use_cache = self.config.use_cache,\n",
    "        )\n",
    "        hidden_states = hidden_states+residual\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = hidden_states+residual\n",
    "        return hidden_states\n",
    " \n",
    "class Qwen2Attention(nn.Module):\n",
    "    def __init__(self,config,layer_idx):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.is_causal = True\n",
    "        if(self.config.head_dim * self.config.num_heads) != self.config.hidden_size:\n",
    "            raise ValueError(f\"hidden_size must be divisible by num_heads!(got 'hidden_size:'{self.config.hidden_size} and 'num_heads:'{self.config.num_heads})\")\n",
    "        self.q_proj = nn.Linear(self.config.hidden_size,self.config.num_heads*self.config.head_dim,bias=self.config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.config.hidden_size,self.config.num_key_value_heads*self.config.head_dim,bias=self.config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.config.hidden_size,self.config.num_key_value_heads*self.config.head_dim,bias=self.config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.config.num_heads*self.config.head_dim,self.config.hidden_size,bias=self.config.attention_bias)\n",
    "        self.rotary_emb = Qwen2RotaryEmbedding(self.config.head_dim,\n",
    "                    max_position_embeddings=self.config.max_position_embeddings,base=self.config.rope_theta)\n",
    "    def forward(self,hidden_states,attention_mask):\n",
    "        bsz,q_len,_=hidden_states.size()\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        query_states = query_states.view(bsz,q_len,self.config.num_heads,self.config.head_dim).transpose(1,2)\n",
    "        key_states = key_states.view(bsz,q_len,self.config.num_key_value_heads,self.config.head_dim).transpose(1,2)\n",
    "        value_states = value_states.view(bsz,q_len,self.config.key_value_heads,self.config.head_dim).transpose(1,2)\n",
    "        cos,sin = self.rotary_emb(value_states,seq_len=self.config.kv_seq_len)\n",
    "        query_states,key_states = apply_rotary_pos_emb(query_states,key_states,cos,sin,self.config.position_ids)\n",
    "        key_states = repeat_kv(key_states,self.config.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states,self.config.num_key_value_groups)\n",
    "        attn_weights = torch.matmul(query_states,key_states.transpose(2,3))/math.sqrt(self.config.head_dim)\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "        attn_weights = nn.functional.softmax(attn_weights,dim=-1,dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights,p=self.config.attention_dropout,training=self.config.training)\n",
    "        attn_output = torch.matmul(attn_weights,value_states)\n",
    "        attn_output = attn_output.transpose(1,2).contiguous()\n",
    "        attn_output = attn_output.reshape(bsz,q_len,self.config.hidden_size)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "def repeat_kv(hidden_states:torch.Tensor,n_rep:int)->torch.Tensor:\n",
    "    batch,num_key_value_heads,slen,head_dim = hidden_states.shape\n",
    "    if n_rep ==1:\n",
    "        return hidden_states\n",
    "    # expand实际并没有复制，只是返回了同一个内存的数据\n",
    "    # repeat会实际复制\n",
    "    hidden_states = hidden_states[:,:,None,:,:].expand(batch,\n",
    "                    num_key_value_heads,n_rep,slen,head_dim) \n",
    "    return hidden_states.reshape(batch,num_key_value_heads*n_rep,slen,head_dim)   \n",
    "\n",
    "class Qwen2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self,dim,max_position_embeddings=2048,base=10000,device=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        # 生成角度theta\n",
    "        inv_freq = 1.0/(self.base)**(torch.arange(0,self.dim,2,dtype=torch.int64).float().to(device)/self.dim)\n",
    "        # 定义一组参数，训练时不会更新，即不会因为optimizer.step()改变（除非动手改变）\n",
    "        # 但在保存模型时会保存这组数据\n",
    "        self.register_buffer(\"inv_freq\",inv_freq,persistent=False)\n",
    "\n",
    "        self._set_cos_sin_cache(seq_len=max_position_embeddings,device=self.inv_freq.device,dtype=torch.get_default_dtype())\n",
    "    \n",
    "    def _set_cos_sin_cache(self,seq_len,device,dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached,device=device,dtype=torch.int64).type_as(self.inv_freq)\n",
    "        # 计算序列序号与角度的内积，即sin和cos内的值\n",
    "        freqs = torch.outer(t,self.inv_freq)\n",
    "        # 因为奇偶是分开的，所以这里重复了一下，这样emb就和x的维度相同了\n",
    "        emb = torch.cat((freqs,freqs),dim=-1)\n",
    "        # 计算cos和sin，并注册数据\n",
    "        self.register_buffer(\"cos_cached\",emb.cos().to(dtype),persistent=False)\n",
    "        self.register_buffer(\"sin_cached\",emb.sin().to(dtype),persistent=False)\n",
    "    \n",
    "    def forward(self,x:torch.Tensor,seq_len=None):\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len,device=x.device,dtype=x.dtype)\n",
    "        return (self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "                self.sin_cached[:seq_len].to(dtype=x.dtype))  \n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[...,:x.shape[-1]//2]\n",
    "    x2 = x[...,x.shape[-1]//2:]\n",
    "    return torch.cat((-x2,x1),dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q,k,cos,sin,position_ids,unsqueeze_dim=1):\n",
    "    \n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q*cos)+(rotate_half(q)*sin)\n",
    "    k_embed = (k*cos)+(rotate_half(k)*sin)\n",
    "    return q_embed,k_embed\n",
    "\n",
    "class Qwen2FlashAttention2(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "    def forward(self):\n",
    "        pass  \n",
    "class Qwen2SdpaAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "    def forward(self):\n",
    "        pass \n",
    "# 这是一个激活函数      \n",
    "ACT2FN = {}    \n",
    "class Qwen2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        \n",
    "        self.gate_proj = nn.Linear(self.hidden_size,self.intermediate_size,bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size,self.intermediate_size,bias=False)\n",
    "        self.down_proj = nn.Linear(self.hidden_size,self.intermediate_size,bias=False)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x))*self.up_proj(x))\n",
    "        return down_proj\n",
    "# 没什么说的，只是在求$RMSNorm(x)=\\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}w_{i}^{2}+\\epsilon}}$     \n",
    "class Qwen2RMSNorm(nn.Module):\n",
    "    def __init__(self,hidden_size,eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "    def forward(self,hidden_states:torch.Tensor):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1,keepdim=True)\n",
    "        hidden_states = hidden_states*torch.rsqrt(variance+self.variance_epsilon)\n",
    "        return self.weight*hidden_states.to(input_dtype)   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
